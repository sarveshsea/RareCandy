{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RareCandy Retrain + Calibration\n",
        "\n",
        "This notebook fits isotonic and logistic calibrators, evaluates calibration quality, tunes confidence thresholds for edge per deployed dollar, and saves calibrated model artifacts for deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from analysis.calibration_utils import (\n",
        "    load_export,\n",
        "    build_calibration_dataset,\n",
        "    split_train_test,\n",
        "    fit_calibrators,\n",
        "    apply_calibrators,\n",
        "    calibration_metrics,\n",
        "    calibration_curve_table,\n",
        "    per_bin_pnl_table,\n",
        "    threshold_sweep,\n",
        ")\n",
        "BASE_DIR = Path.cwd()\n",
        "EXPORTS_DIR = BASE_DIR / \"exports\"\n",
        "OUT_DIR = BASE_DIR / \"analysis\" / \"artifacts\" / \"calibration\"\n",
        "MODEL_DIR = OUT_DIR / \"models\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"exports:\", EXPORTS_DIR)\n",
        "print(\"out:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Load export and build calibration dataset\n",
        "raw_df = load_export(EXPORTS_DIR, \"rarecandy_export\")\n",
        "dataset = build_calibration_dataset(raw_df, horizon=1, trading_cost=0.0006)\n",
        "train_df, test_df = split_train_test(dataset, train_frac=0.7)\n",
        "\n",
        "print(\"rows_total:\", len(raw_df))\n",
        "print(\"rows_used:\", len(dataset.frame))\n",
        "print(\"rows_train:\", len(train_df), \"rows_test:\", len(test_df))\n",
        "display(dataset.frame.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Fit calibrators and score holdout\n",
        "calibrators = fit_calibrators(train_df)\n",
        "scored = apply_calibrators(test_df, calibrators)\n",
        "\n",
        "y_true = scored[\"label\"].to_numpy(dtype=float)\n",
        "metric_raw = calibration_metrics(y_true, scored[\"prob_raw\"].to_numpy(dtype=float), n_bins=10)\n",
        "metric_iso = calibration_metrics(y_true, scored[\"prob_isotonic\"].to_numpy(dtype=float), n_bins=10)\n",
        "metric_log = calibration_metrics(y_true, scored[\"prob_logistic\"].to_numpy(dtype=float), n_bins=10)\n",
        "\n",
        "metrics_df = pd.DataFrame([\n",
        "    {\"model\": \"raw\", **metric_raw},\n",
        "    {\"model\": \"isotonic\", **metric_iso},\n",
        "    {\"model\": \"logistic\", **metric_log},\n",
        "]).sort_values([\"ece\", \"brier\"])\n",
        "display(metrics_df)\n",
        "\n",
        "selected_model = metrics_df.iloc[0][\"model\"]\n",
        "selected_col = {\"raw\": \"prob_raw\", \"isotonic\": \"prob_isotonic\", \"logistic\": \"prob_logistic\"}[selected_model]\n",
        "print(\"selected_model:\", selected_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Calibration curve + per-bin P&L + threshold tuning\n",
        "curve_raw = calibration_curve_table(y_true, scored[\"prob_raw\"], n_bins=10)\n",
        "curve_iso = calibration_curve_table(y_true, scored[\"prob_isotonic\"], n_bins=10)\n",
        "curve_log = calibration_curve_table(y_true, scored[\"prob_logistic\"], n_bins=10)\n",
        "\n",
        "bin_pnl = per_bin_pnl_table(scored, selected_col, n_bins=10)\n",
        "thresholds = threshold_sweep(scored, selected_col, threshold_min=0.50, threshold_max=0.95, threshold_step=0.01, min_signals=20)\n",
        "\n",
        "display(bin_pnl)\n",
        "display(thresholds.head(10))\n",
        "\n",
        "best_threshold = thresholds.iloc[0].to_dict() if not thresholds.empty else None\n",
        "print(\"best_threshold:\", best_threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Save calibrated artifacts\n",
        "joblib.dump(calibrators[\"isotonic\"], MODEL_DIR / \"isotonic_calibrator.joblib\")\n",
        "joblib.dump(calibrators[\"logistic\"], MODEL_DIR / \"logistic_calibrator.joblib\")\n",
        "(MODEL_DIR / \"selected_model.txt\").write_text(selected_model + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "curve_raw.to_csv(OUT_DIR / \"calibration_curve_raw.csv\", index=False)\n",
        "curve_iso.to_csv(OUT_DIR / \"calibration_curve_isotonic.csv\", index=False)\n",
        "curve_log.to_csv(OUT_DIR / \"calibration_curve_logistic.csv\", index=False)\n",
        "bin_pnl.to_csv(OUT_DIR / \"per_bin_pnl.csv\", index=False)\n",
        "thresholds.to_csv(OUT_DIR / \"threshold_sweep.csv\", index=False)\n",
        "\n",
        "report = {\n",
        "    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "    \"selected_model\": selected_model,\n",
        "    \"selected_model_metrics\": metrics_df.iloc[0].to_dict(),\n",
        "    \"best_threshold\": best_threshold,\n",
        "    \"rows_test\": int(len(scored)),\n",
        "}\n",
        "(OUT_DIR / \"calibration_report_from_notebook.json\").write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Saved models and calibration artifacts under:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Optional: run standalone report + alerts scripts from notebook\n",
        "# !python3 analysis/calibration_report.py\n",
        "# !python3 analysis/check_calibration_alerts.py\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}